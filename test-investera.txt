Написать парсер статей с википедии(рекурсивно(уровень вложенности - 5)), сохранять результаты в бд. Затем брать уже спарсенные статьи из базыи просить написать summary для них у нейронки(deepseek, chat-gpt) и тд(использовать любое api)

flow работы:
1. Запрос на api для парсинга статьи(можно ссылкой)
2. Сохранение результатов в базу(для статьи и вложенных статей)
3. Затем генерация summary статей(исходная + вложенные) 
4. После генерации статей в базе должно создаваться запись исходная статья - ее summary(для вложенных статей summary генерить не надо)

Реализация:
Реляционая бд: Postgres
web Фрэймворк: Fastapi
Остальное полностью на усмотрение(структура базы, архитектура и тд), НО: все должно выполняться в асинхронной среде

Будет плюсом: 
1. Поднять все через докер
2. Использование di
3. Грамотная декомпозиция логики
4. Использование service и repository layers


создать два эндпоинта:
1. Получение summary статьи по ее url
2. Запуск flow парсинга по url


Результат работы выложить на гит, рекрутеру необходимо отправить ссылку на репозиторий
